name: "lora_default"
type: "lora"
num_layers: 4
learning_rate: 5e-5
batch_size: 16
iterations: 50
fine-tune-type: "full"
adapter_save_path: "./adapters/{model_name}_{data_name}"

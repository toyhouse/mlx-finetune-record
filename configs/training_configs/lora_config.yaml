name: "lora_default"
type: "lora"
num_layers: 4
learning_rate: 5e-6
batch_size: 8
iterations: 200
fine-tune-type: "lora"
adapter_save_path: "./adapters/{model_name}_{data_name}"
